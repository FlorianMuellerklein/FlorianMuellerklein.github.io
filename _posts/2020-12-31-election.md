---
layout: post
title: The Case for Uncertainty and Simple Models in Election Forecasting
published: False
---

After all of the post motem conversations about election forecast we once again hear a lot about how off the polls were. So the goal here was to create a simple election forecasting system by only using polling data, historical election results, and incorporating sufficient uncertainty. The resulting model seems like a good baseline to compare others against. 

The basis of the model is super simple. Start with historical election results by state, load up all of the polling data for each state. Figure out how the polling suggests to alter the historical trend of the states and use that to simulate the Electoral College. 

The results are quite interesting. The model does well even though it is extremely simple, makes very few assumptions, only uses state level polling, doesn't use any fundamentals, and no state correlations. The forecasted electoral college votes are closer to reality in some elections and the forecasted win probabilites tend to be lower than other models.

What is also wild to me is that such a simple method gets similar results to other models which seem way more complicated. Perhaps we should all be starting with simple methods to establish baselines before getting crazy with more advanced (and more fun) methods. 

## Why Uncertainty?

Maybe to some it will seem like a cop-out but I believe that adding uncertainty is the correct assumption when dealing with human behavior. Models have to be simplifying and when we simplify humans by placing them into large groups we should probably add some additional uncertainty.

Additionally, when doing some research I couldn't help but end up on computational finance blogs and websites that discuss how fatter tail distributions are more appropriate for modeling stock returns. Specifically to have their models be much less brittle to big unexpected swings. I think a similar case can be made for elections. 

Additionally, using normal distributions has a stronger containment effect than the Student T. Which means that if we have historical election information in our prior and the polls suggest that something different may happen, the normal distribution will prevent the model from shiftign with the data more than the StudentT.

### Why Student T?

For a simple illustrative example take Florida over the past 5 elections (2000 - 2016)for example. If we average the popular vote margins over those elections we get roughly 0.12% (toward dems) and with an average swing of 2.4%. This year in 2020 Trump won Florida by a margin of about 3.3%. A normal distribution using these parameters would give us a probability of 7% of that or a greater margin happening. Whereas a Student T distribution gives us a probability of 19.5%. It seems like we would want to allow for such a swing in our models.

{: .center}
![](../images/normal_vs_t.png)

We can see from this example that its not just about adding uncertainty to cover our butts if something unexpected happens, it also produces more realistic probabilites for events that do happen. 

### Normal or Beta Priors

## My Assumptions

I am by no means a polling or political expert, not by a long shot. So I tried to keep my assumptions low to avoid injecting my own biases into the model. Instead my goal is to include enough uncertainty in the model that the outputs are reasonable when backtesting against known election outcomes. 

1. **Polls** No poll corrections/adjustments. I am no expert in polling so it would be detrimental for me to introduce my own biases into that data. Polling is hard work and the majority of pollsters are working really hard to provide accurate numbers. It seems like compounding my own corrections onto theirs could lead to problems. <br/><br/> I have a hunch that even bad polls could be useful in modern times. There could be market forces that award biased polls and the proportions of those biased polls could reflect the proportion of the population that has those biases.   

2. **Temporal Effects** Polls closer to the election date should be relied on more heavily. 

3. **Priors** Use data from the past 5 elections to develop priors for each state. With each election weighted by how long ago it was. 

## Results

Results use all of the polling data up until Nov 2 on each election year. 

#### Democrat Win Probability 
| Year  | 538    | Princeton  | Illinois        | Other           | Mine | 
| ------|:------:|:----------:|:----------:     | :----------:    |:----------:|
| 2020 | 89%     | NA         |  99.9%          | 97% (Economist) | 86.3%  |
| 2016 | 71.4%   | 93%        |  95.3%          | 85% (NYT)       | 87.5% |
| 2012 | 90.9%   | NA         |  98.3%          |                 | 94.6% |
| 2008 |   NA    | NA         |  99.8%          |                 | 76.4%* |

#### Democrat Mean Electoral College Prediction 
| Year  | 538    | Princeton  | Illinois   |Other           | Mine       | Result |
| ------|:------:|:----------:|:----------:|:----------:    |:----------:|:----------:|
| 2020  | 348    | 342        |  350       | 356 (Economist)|  316    | 306 |
| 2016  | 302.2  | 323        |  297       | 322 (NYT)      |  297    | 232 |
| 2012  | 332    | 305        |  307       | 281 (270 2w)   |  306    | 332 |
| 2008  | 349    | 352        |  354.6     |                |  295    | 365 |

* Polls from 2012 are from [wikipedia](https://en.wikipedia.org/wiki/Statewide_opinion_polling_for_the_2012_United_States_presidential_election). There are significantly less polls there than in the other two datasets, 668 polls in 2012 vs 6020 for 2020. If anyone has a bigger dataset for 2012 let me know! I imagine that this method would be more in line with the others with a proper polling datset. 

* The 270 to Win forecast came from a [crowdsourced contest](https://www.270towin.com/americas-electoral-map/), which is kind of awesome actually. 

* The [Illinois model](https://electionanalytics.cs.illinois.edu/site/file/Rigdon2009.pdf) is quite similar to mine, however the major difference is in the distributions used in the state modeling step. https://electionanalytics.cs.illinois.edu/index.php 


## Methods

* State level win probabilities come from Bayesian estimators that use historical election results as priors and get updated with aggregated polling data. This results in a posterior estimation of the popular vote margins for each state, from that we get the probability of a win for each party. 

* The state probabilities are used in a Monte Carlo simulation to determine the potential distributions of electoral votes for each candidate.

### State Level

State level model assumes that the true vote margin is distributed by a Student T distribution. We start from a weighted average of the previous 5 elections, giving more weight to more recent elections. Then allow the data to produce the forecasted distribution of margins. However we can not know the true vote margin and there are probably more reasons why then we can list. For example, are the same subset of Florida residents voting in each election, are some voters changing their preferred party between elections, or did some crazy event happen that made someone change their mind last minute?

We assume that our window is through the polling data. But that comes with its own issues and I try to handle those through the fat tailed distributions. Additionally, I assume (correctly or incorrectly) that polling errors are distributed around 0 with an average swing of about 4% (from historical [538 analysis](https://fivethirtyeight.com/features/the-polls-are-all-right/)). 

So the state model looks like the following. Where *d* represents the distribution of vote margins. Then assume that the sample mean (polling data) is dependent on the actual margins. We then want the distribution of polling to capture the randomness of sampling, public opinion, life events interferring with pollsters, distrust of polling institutions, distrust of establishments, lies to pollsters, you name it, and finally adds in historical polling error explicitly.  

{: .center}
![](../images/prior.gif)

{: .center}
![](../images/likelihood.gif)

### Electoral College

The electoral college winners are estimated by Monte Carlo simulation. Taking the estimated win probabilities from the state models and using them for thousands of independent coin flips per state. Proportion of electoral votes in the simulations above 270 give us the probability of the winner. 


### 2020 Map and EC Sims

{% include state_preds_2020.html %}

{: .center}
![](../images/ec_vote_sims_2020-11-02-polls.png)

### 2016 Map and EC Sims

{% include state_preds_2016.html %}

{: .center}
![](../images/ec_vote_sims_2016-11-02-adjpolls.png)

### 2012 Map and EC Sims

{% include state_preds_2012.html %}

{: .center}
![](../images/ec_vote_sims_2012-11-02-polls.png)

### 2008 Map and EC Sims

{% include state_preds_2008.html %}

{: .center}
![](../images/ec_vote_sims_2008-11-02-polls.png)

## Conclusions

1) A simple model can do quite well as long as the distributions used provide realistic probabilities of different events happening. 

2) It seems that we can safely say that the polls are useful for forecasting the winner of the presidential election. The pollsters definitely fixed something from 2016 to 2020 as the result of the 2020 is more realistic than from 2016. 

3) Creating a simple model first and iterating from there would likely lead to better forecasts in the future. 

## References

### Bayesian and Sim Methods
[Predicting the Next US President by Simulating the Electoral College](https://scholarship.claremont.edu/jhm/vol8/iss1/5/)

[Bayesian Coin Flips](https://www.thomasjpfan.com/2015/09/bayesian-coin-flips/)

[Introduction to Data Science: Data Analysis and Predicton Algorithms with R](https://rafalab.github.io/dsbook/models.html#election-forecasting) Describes a useful hierarchical bayesian model that incorporates historical data and poll aggregations. 

### StudentT
[Robust Statistical Modeling Using the t Distribution](https://escholarship.org/content/qt27s1d3h7/qt27s1d3h7.pdf)"If *v* (degrees of freedom in studentT) is fixed a prior at some reasonable value, it is a robustness tuning parameter." In other words we can use it as a hyperparameter that we control to decide how robust we want our model to be against extreme values. Here the parameter was tuned through back-testing on past elections. 

[What is the probability that your vote will make a difference?](https://www.nber.org/system/files/working_papers/w15220/w15220.pdf)"The t distribution with 4 degrees of freedom is commonly used as a robust alternative to the normal (Lange, Little, and Taylor, 1989); we use it here to allow for the possibility of unanticipated shocks. Using the t instead of the normal has little effect on the probability of a decisive vote in close states, but it moderates the results in states farther from the national median , for example changing the estimated probability of decisiveness in the District of Columbia from 5e-85 to 2e-12, in Utah from 9e-16 to 2e-10, but changing the estimate in Ohio only from 3.4e-8 to 3.1e-8."

### Cauchy Dist (T dist with degrees of freedom = 1)
[Why heavy tails](http://swer.wtamu.edu/sites/default/files/Data/swer%2014%20Harris.pdf)
"The Cauchy distribution, just like the Gaussian distribution, is a stable distribution, but one with fat tails. Over the years several academic papers have looked into it as an alternative to best describe these extreme volatility events." 

[Stock option pricing inference](https://thomasvilhena.com/2019/12/stock-option-pricing-inference)
"Heavy-tailed distributions, like Cauchy, are better models for financial returns because the normal model does not capture the large fluctuations seen in real assets." 

[Goodness-of-fit testing for the Cauchy distribution with application to financial modeling](https://www.sciencedirect.com/science/article/pii/S1018364718313193)

### Machine Learning and Sims
[Forecasting the 2020 US Elections with Decision Desk HQ: Methodology for Modern American Electoral Dynamics](https://hdsr.mitpress.mit.edu/pub/gach7e59/release/1)